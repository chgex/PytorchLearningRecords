{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0ac4d70e4d583a84b7ad989b773b88f6bded725ad52ee17c9fe1a30f4567fa143",
   "display_name": "Python 3.8.8 64-bit ('pytorchGPU': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CH09-Torchvision目标检测微调\n",
    "\n",
    "[官方链接](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n",
    "\n",
    "\n",
    "数据集使用Penn-Fudan，它包含170张图片和345个行人示例，文件结构如下：\n",
    "```\n",
    "PennFudanPed/\n",
    "  PedMasks/\n",
    "    FudanPed00001_mask.png\n",
    "    FudanPed00002_mask.png\n",
    "    ...\n",
    "  PNGImages/\n",
    "    FudanPed00001.png\n",
    "    FudanPed00002.png\n",
    "    ...\n",
    "```\n",
    "每张图片，对应一个分割mask，其中每种颜色对应不同的实例。\n",
    "\n",
    "\n",
    "## 1 为数据集自定义Dataset类\n",
    "\n",
    "自定义的Dataset类继承自`torch.utils.data.Dataset`类，自定义该类需要实现`__getitem__`和`__len__`方法。其中，`__getitem__`应该return以下东西:\n",
    "\n",
    "+ image: 尺寸为(H,W)的PIL图像\n",
    "\n",
    "+ target: 字典，包含以下内容：\n",
    "\n",
    "    + `boxes (FloatTensor[N, 4])`: N为边界框个数，边界框坐标[x0, y0, x1, y1];\n",
    "\n",
    "    + `labels (Int64Tensor[N])`: N个边界框对于的label。一般情况下背景的label为0;\n",
    "    + `image_id (Int64Tensor[1])`: 图片索引，在数据集中，每个图片对应唯一的索引;\n",
    "    \n",
    "    + `area (Tensor[N])`: 边界框的面积.在COCO指标评价中，需要用到这个参数，划分small, medium and large boxes.\n",
    "\n",
    "    + `iscrowd (UInt8Tensor[N])`: iscrowd=True，则在评估中，忽略;\n",
    "\n",
    "    + `(optionally) masks (UInt8Tensor[N, H, W])`: The segmentation masks for each one of the objects\n",
    "\n",
    "可选：\n",
    "```\n",
    "    + (optionally) keypoints (FloatTensor[N, K, 3]): For each one of the N objects, it contains the K keypoints in [x, y, visibility] format, defining the object. visibility=0 means that the keypoint is not visible. \n",
    "    Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt references/detection/transforms.py for your new keypoint representation\n",
    "```\n",
    "\n",
    "如果设置了纵横比，需自定义`get_height_and_width`方法，该方法会返回图像的高度和宽度，否则，DataLoader时，将自动查询数据集的所有元素`__getitem__,`，但是这样将在内存中加载图像，导致速度降低。\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = Image.open(mask_path)\n",
    "        # convert the PIL Image into a numpy array\n",
    "        mask = np.array(mask)\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # convert everything into a torch.Tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # there is only one class\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "source": [
    "## 2 定义模型\n",
    "\n",
    "使用Mask Rcnn模型，该模型基于Fast Rcnn，两者的结构图分别如下：\n",
    "\n",
    "fast rcnn结构图： \n",
    "\n",
    "![fast rcnn](./data_ch09/FastRcnn.png)\n",
    "\n",
    "mask rcnn结构图：\n",
    "\n",
    "![mask rcnn](./data_ch09/MaskRcnn.png)\n",
    "\n",
    "使用在COCO数据集上预训练的模型，基于该模型，进行微调："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to C:\\Users\\admin/.cache\\torch\\hub\\checkpoints\\fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# load a model pre-trained pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "source": [
    "### 2.1 修改模型以添加不同的骨干网络"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\admin/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# load a pre-trained model for classification and return\n",
    "# only the features\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "\n",
    "# FasterRCNN needs to know the number of\n",
    "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
    "# so we need to add it here\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# let's make the RPN generate 5 x 3 anchors per spatial\n",
    "# location, with 5 different sizes and 3 different aspect\n",
    "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
    "# map could potentially have different sizes and\n",
    "# aspect ratios\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# let's define what are the feature maps that we will\n",
    "# use to perform the region of interest cropping, as well as\n",
    "# the size of the crop after rescaling.\n",
    "# if your backbone returns a Tensor, featmap_names is expected to\n",
    "# be [0]. More generally, the backbone should return an\n",
    "# OrderedDict[Tensor], and in featmap_names you can choose which\n",
    "# feature maps to use.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# put the pieces together inside a FasterRCNN model\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "source": [
    "### 2.2 定义mask rcnn 模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "source": [
    "## 3 将以上步骤组合在一起 \n",
    "\n",
    "训练函数和验证函数使用了data_ch09下的engine.py和utils.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_ch09 import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total time: 0:00:06 (0.1281 s / it)\n",
      "Averaged stats: model_time: 0.1087 (0.1100)  evaluator_time: 0.0020 (0.0043)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.829\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.994\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.963\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.697\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.844\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.344\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.868\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.792\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.879\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.769\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.994\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.938\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.593\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.781\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.313\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.803\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.803\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.750\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.812\n",
      "Epoch: [6]  [ 0/60]  eta: 0:00:36  lr: 0.000050  loss: 0.1719 (0.1719)  loss_classifier: 0.0255 (0.0255)  loss_box_reg: 0.0435 (0.0435)  loss_mask: 0.1015 (0.1015)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0008 (0.0008)  time: 0.6154  data: 0.0229  max mem: 3599\n",
      "Epoch: [6]  [10/60]  eta: 0:00:31  lr: 0.000050  loss: 0.1733 (0.1731)  loss_classifier: 0.0248 (0.0249)  loss_box_reg: 0.0372 (0.0364)  loss_mask: 0.1041 (0.1094)  loss_objectness: 0.0004 (0.0006)  loss_rpn_box_reg: 0.0014 (0.0018)  time: 0.6304  data: 0.0250  max mem: 3599\n",
      "Epoch: [6]  [20/60]  eta: 0:00:23  lr: 0.000050  loss: 0.1636 (0.1671)  loss_classifier: 0.0212 (0.0229)  loss_box_reg: 0.0260 (0.0329)  loss_mask: 0.1024 (0.1086)  loss_objectness: 0.0004 (0.0008)  loss_rpn_box_reg: 0.0014 (0.0020)  time: 0.5964  data: 0.0247  max mem: 3599\n",
      "Epoch: [6]  [30/60]  eta: 0:00:18  lr: 0.000050  loss: 0.1478 (0.1672)  loss_classifier: 0.0196 (0.0235)  loss_box_reg: 0.0268 (0.0327)  loss_mask: 0.1009 (0.1079)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0017 (0.0022)  time: 0.5990  data: 0.0275  max mem: 3599\n",
      "Epoch: [6]  [40/60]  eta: 0:00:12  lr: 0.000050  loss: 0.1656 (0.1769)  loss_classifier: 0.0226 (0.0256)  loss_box_reg: 0.0309 (0.0361)  loss_mask: 0.1070 (0.1117)  loss_objectness: 0.0005 (0.0008)  loss_rpn_box_reg: 0.0029 (0.0026)  time: 0.6452  data: 0.0303  max mem: 3599\n",
      "Epoch: [6]  [50/60]  eta: 0:00:06  lr: 0.000050  loss: 0.1691 (0.1756)  loss_classifier: 0.0255 (0.0253)  loss_box_reg: 0.0340 (0.0357)  loss_mask: 0.1070 (0.1111)  loss_objectness: 0.0004 (0.0009)  loss_rpn_box_reg: 0.0029 (0.0027)  time: 0.6418  data: 0.0305  max mem: 3599\n",
      "Epoch: [6]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1576 (0.1765)  loss_classifier: 0.0251 (0.0253)  loss_box_reg: 0.0340 (0.0359)  loss_mask: 0.1099 (0.1113)  loss_objectness: 0.0004 (0.0012)  loss_rpn_box_reg: 0.0025 (0.0028)  time: 0.6199  data: 0.0285  max mem: 3599\n",
      "Epoch: [6] Total time: 0:00:37 (0.6200 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/50]  eta: 0:00:06  model_time: 0.1107 (0.1107)  evaluator_time: 0.0060 (0.0060)  time: 0.1356  data: 0.0180  max mem: 3599\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.1087 (0.1103)  evaluator_time: 0.0020 (0.0042)  time: 0.1270  data: 0.0128  max mem: 3599\n",
      "Test: Total time: 0:00:06 (0.1285 s / it)\n",
      "Averaged stats: model_time: 0.1087 (0.1103)  evaluator_time: 0.0020 (0.0042)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.835\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.994\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.947\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.684\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.849\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.344\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.874\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.874\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.800\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.886\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.766\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.994\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.938\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.594\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.778\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.311\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.801\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.801\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.750\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.809\n",
      "Epoch: [7]  [ 0/60]  eta: 0:00:34  lr: 0.000050  loss: 0.1113 (0.1113)  loss_classifier: 0.0081 (0.0081)  loss_box_reg: 0.0078 (0.0078)  loss_mask: 0.0941 (0.0941)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0012 (0.0012)  time: 0.5711  data: 0.0309  max mem: 3599\n",
      "Epoch: [7]  [10/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1669 (0.1662)  loss_classifier: 0.0219 (0.0220)  loss_box_reg: 0.0355 (0.0347)  loss_mask: 0.1082 (0.1067)  loss_objectness: 0.0002 (0.0005)  loss_rpn_box_reg: 0.0015 (0.0023)  time: 0.5973  data: 0.0252  max mem: 3599\n",
      "Epoch: [7]  [20/60]  eta: 0:00:24  lr: 0.000050  loss: 0.1597 (0.1682)  loss_classifier: 0.0222 (0.0239)  loss_box_reg: 0.0269 (0.0331)  loss_mask: 0.1034 (0.1082)  loss_objectness: 0.0003 (0.0008)  loss_rpn_box_reg: 0.0021 (0.0022)  time: 0.6042  data: 0.0247  max mem: 3599\n",
      "Epoch: [7]  [30/60]  eta: 0:00:18  lr: 0.000050  loss: 0.1788 (0.1799)  loss_classifier: 0.0282 (0.0274)  loss_box_reg: 0.0319 (0.0373)  loss_mask: 0.1096 (0.1116)  loss_objectness: 0.0004 (0.0008)  loss_rpn_box_reg: 0.0024 (0.0029)  time: 0.6128  data: 0.0288  max mem: 3599\n",
      "Epoch: [7]  [40/60]  eta: 0:00:12  lr: 0.000050  loss: 0.1663 (0.1751)  loss_classifier: 0.0271 (0.0258)  loss_box_reg: 0.0304 (0.0349)  loss_mask: 0.1112 (0.1110)  loss_objectness: 0.0003 (0.0007)  loss_rpn_box_reg: 0.0023 (0.0027)  time: 0.6100  data: 0.0291  max mem: 3599\n",
      "Epoch: [7]  [50/60]  eta: 0:00:06  lr: 0.000050  loss: 0.1571 (0.1803)  loss_classifier: 0.0234 (0.0263)  loss_box_reg: 0.0257 (0.0361)  loss_mask: 0.1077 (0.1143)  loss_objectness: 0.0003 (0.0007)  loss_rpn_box_reg: 0.0020 (0.0028)  time: 0.6066  data: 0.0272  max mem: 3599\n",
      "Epoch: [7]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1639 (0.1783)  loss_classifier: 0.0267 (0.0262)  loss_box_reg: 0.0300 (0.0353)  loss_mask: 0.1058 (0.1134)  loss_objectness: 0.0003 (0.0007)  loss_rpn_box_reg: 0.0018 (0.0027)  time: 0.6310  data: 0.0284  max mem: 3599\n",
      "Epoch: [7] Total time: 0:00:36 (0.6156 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/50]  eta: 0:00:06  model_time: 0.1097 (0.1097)  evaluator_time: 0.0060 (0.0060)  time: 0.1346  data: 0.0189  max mem: 3599\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.1077 (0.1100)  evaluator_time: 0.0020 (0.0042)  time: 0.1266  data: 0.0125  max mem: 3599\n",
      "Test: Total time: 0:00:06 (0.1279 s / it)\n",
      "Averaged stats: model_time: 0.1077 (0.1100)  evaluator_time: 0.0020 (0.0042)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.834\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.994\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.948\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.680\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.346\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.873\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.873\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.792\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.885\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.769\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.994\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.938\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.595\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.781\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.314\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.802\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.802\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.750\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.811\n",
      "Epoch: [8]  [ 0/60]  eta: 0:00:37  lr: 0.000050  loss: 0.2056 (0.2056)  loss_classifier: 0.0382 (0.0382)  loss_box_reg: 0.0408 (0.0408)  loss_mask: 0.1199 (0.1199)  loss_objectness: 0.0012 (0.0012)  loss_rpn_box_reg: 0.0055 (0.0055)  time: 0.6298  data: 0.0359  max mem: 3599\n",
      "Epoch: [8]  [10/60]  eta: 0:00:29  lr: 0.000050  loss: 0.1704 (0.1709)  loss_classifier: 0.0262 (0.0247)  loss_box_reg: 0.0364 (0.0337)  loss_mask: 0.1107 (0.1092)  loss_objectness: 0.0004 (0.0005)  loss_rpn_box_reg: 0.0023 (0.0027)  time: 0.5906  data: 0.0261  max mem: 3599\n",
      "Epoch: [8]  [20/60]  eta: 0:00:24  lr: 0.000050  loss: 0.1704 (0.1880)  loss_classifier: 0.0264 (0.0280)  loss_box_reg: 0.0364 (0.0407)  loss_mask: 0.1020 (0.1149)  loss_objectness: 0.0005 (0.0014)  loss_rpn_box_reg: 0.0028 (0.0030)  time: 0.6244  data: 0.0285  max mem: 3599\n",
      "Epoch: [8]  [30/60]  eta: 0:00:18  lr: 0.000050  loss: 0.1642 (0.1812)  loss_classifier: 0.0243 (0.0265)  loss_box_reg: 0.0287 (0.0375)  loss_mask: 0.1019 (0.1130)  loss_objectness: 0.0005 (0.0013)  loss_rpn_box_reg: 0.0028 (0.0029)  time: 0.6392  data: 0.0301  max mem: 3599\n",
      "Epoch: [8]  [40/60]  eta: 0:00:12  lr: 0.000050  loss: 0.1600 (0.1806)  loss_classifier: 0.0227 (0.0258)  loss_box_reg: 0.0262 (0.0362)  loss_mask: 0.1056 (0.1148)  loss_objectness: 0.0004 (0.0011)  loss_rpn_box_reg: 0.0017 (0.0027)  time: 0.5997  data: 0.0275  max mem: 3599\n",
      "Epoch: [8]  [50/60]  eta: 0:00:06  lr: 0.000050  loss: 0.1550 (0.1748)  loss_classifier: 0.0211 (0.0251)  loss_box_reg: 0.0259 (0.0344)  loss_mask: 0.1015 (0.1117)  loss_objectness: 0.0004 (0.0010)  loss_rpn_box_reg: 0.0018 (0.0026)  time: 0.5849  data: 0.0261  max mem: 3599\n",
      "Epoch: [8]  [59/60]  eta: 0:00:00  lr: 0.000050  loss: 0.1560 (0.1755)  loss_classifier: 0.0195 (0.0250)  loss_box_reg: 0.0260 (0.0350)  loss_mask: 0.1063 (0.1120)  loss_objectness: 0.0003 (0.0009)  loss_rpn_box_reg: 0.0019 (0.0026)  time: 0.6180  data: 0.0263  max mem: 3599\n",
      "Epoch: [8] Total time: 0:00:36 (0.6151 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/50]  eta: 0:00:06  model_time: 0.1101 (0.1101)  evaluator_time: 0.0060 (0.0060)  time: 0.1360  data: 0.0189  max mem: 3599\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.1078 (0.1097)  evaluator_time: 0.0030 (0.0043)  time: 0.1266  data: 0.0125  max mem: 3599\n",
      "Test: Total time: 0:00:06 (0.1278 s / it)\n",
      "Averaged stats: model_time: 0.1078 (0.1097)  evaluator_time: 0.0030 (0.0043)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.836\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.994\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.963\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.680\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.851\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.874\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.874\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.792\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.886\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.767\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.994\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.938\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.587\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.780\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.313\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.802\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.802\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.742\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.811\n",
      "Epoch: [9]  [ 0/60]  eta: 0:00:35  lr: 0.000005  loss: 0.1716 (0.1716)  loss_classifier: 0.0250 (0.0250)  loss_box_reg: 0.0471 (0.0471)  loss_mask: 0.0983 (0.0983)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0010 (0.0010)  time: 0.5958  data: 0.0329  max mem: 3599\n",
      "Epoch: [9]  [10/60]  eta: 0:00:30  lr: 0.000005  loss: 0.1584 (0.1631)  loss_classifier: 0.0243 (0.0226)  loss_box_reg: 0.0269 (0.0328)  loss_mask: 0.1006 (0.1054)  loss_objectness: 0.0003 (0.0004)  loss_rpn_box_reg: 0.0013 (0.0019)  time: 0.6000  data: 0.0269  max mem: 3599\n",
      "Epoch: [9]  [20/60]  eta: 0:00:24  lr: 0.000005  loss: 0.1462 (0.1657)  loss_classifier: 0.0211 (0.0236)  loss_box_reg: 0.0269 (0.0330)  loss_mask: 0.1006 (0.1063)  loss_objectness: 0.0003 (0.0005)  loss_rpn_box_reg: 0.0016 (0.0022)  time: 0.6141  data: 0.0279  max mem: 3599\n",
      "Epoch: [9]  [30/60]  eta: 0:00:18  lr: 0.000005  loss: 0.1674 (0.1742)  loss_classifier: 0.0269 (0.0257)  loss_box_reg: 0.0311 (0.0358)  loss_mask: 0.1063 (0.1094)  loss_objectness: 0.0005 (0.0009)  loss_rpn_box_reg: 0.0021 (0.0024)  time: 0.6313  data: 0.0313  max mem: 3599\n",
      "Epoch: [9]  [40/60]  eta: 0:00:12  lr: 0.000005  loss: 0.1891 (0.1774)  loss_classifier: 0.0301 (0.0264)  loss_box_reg: 0.0403 (0.0371)  loss_mask: 0.1096 (0.1100)  loss_objectness: 0.0006 (0.0010)  loss_rpn_box_reg: 0.0027 (0.0028)  time: 0.6220  data: 0.0296  max mem: 3599\n",
      "Epoch: [9]  [50/60]  eta: 0:00:06  lr: 0.000005  loss: 0.1769 (0.1758)  loss_classifier: 0.0251 (0.0254)  loss_box_reg: 0.0337 (0.0368)  loss_mask: 0.1096 (0.1098)  loss_objectness: 0.0007 (0.0010)  loss_rpn_box_reg: 0.0027 (0.0027)  time: 0.5953  data: 0.0238  max mem: 3599\n",
      "Epoch: [9]  [59/60]  eta: 0:00:00  lr: 0.000005  loss: 0.1473 (0.1742)  loss_classifier: 0.0180 (0.0247)  loss_box_reg: 0.0249 (0.0357)  loss_mask: 0.0999 (0.1101)  loss_objectness: 0.0003 (0.0009)  loss_rpn_box_reg: 0.0018 (0.0027)  time: 0.6033  data: 0.0238  max mem: 3599\n",
      "Epoch: [9] Total time: 0:00:36 (0.6136 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/50]  eta: 0:00:06  model_time: 0.1127 (0.1127)  evaluator_time: 0.0060 (0.0060)  time: 0.1386  data: 0.0190  max mem: 3599\n",
      "Test:  [49/50]  eta: 0:00:00  model_time: 0.1087 (0.1109)  evaluator_time: 0.0030 (0.0044)  time: 0.1278  data: 0.0132  max mem: 3599\n",
      "Test: Total time: 0:00:06 (0.1292 s / it)\n",
      "Averaged stats: model_time: 0.1087 (0.1109)  evaluator_time: 0.0030 (0.0044)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.836\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.994\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.963\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.680\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.851\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.874\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.874\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.792\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.886\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.763\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.994\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.938\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.583\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.778\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.312\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.799\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.799\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.600\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.742\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.808\n",
      "That's it!\n"
     ]
    }
   ],
   "source": [
    "from data_ch09 import engine,utils\n",
    "\n",
    "# train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "num_classes = 2\n",
    "# use our dataset and defined transformations\n",
    "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
    "dataset_test = PennFudanDataset('data/PennFudanPed', get_transform(train=False))\n",
    "\n",
    "# split the dataset in train and test set\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# define training and validation data loaders\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=0,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "\n",
    "\n",
    "# get the model using our helper function\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    engine.train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    engine.evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}