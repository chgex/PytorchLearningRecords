{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0ac4d70e4d583a84b7ad989b773b88f6bded725ad52ee17c9fe1a30f4567fa143",
   "display_name": "Python 3.8.8 64-bit ('pytorchGPU': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# CH06-TORCH.AUTOGRAD\n",
    "\n",
    "官方链接：https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#\n",
    "\n",
    "\n",
    "在训练神经网络时，最常用的算法是 反向传播（back propagation） 算法，在该算法中，参数（model weights）根据损失函数的梯度进行调整。\n",
    "\n",
    "为了计算这些梯度，PyTorch中有一个名为`torch.autograd()` 的内置微分引擎，它支持任何计算图的梯度计算。\n",
    "\n",
    "考虑最简单的一层神经网络：具有输入x、参数w和b，以及损失函数，它可以通过以下方式定义："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "source": [
    "## 1 张量，Function类和计算图\n",
    "\n",
    "上述代码，计算图如下：\n",
    "\n",
    "![img](./data_ch06/compute-graph.jpg)\n",
    "\n",
    "\n",
    "其中，w和b是模型参数，是需要我们优化的参数。\n",
    "\n",
    "为了能计算出这些变量关于损失的梯度，需要设置tensor的`requires_grad`属性：\n",
    "\n",
    "+ requires_grad可以在创建tensor的时候使用\n",
    "\n",
    "+ 也可以通过`x.requires_grad_(True)`后设置\n",
    "\n",
    "应用在张量上的，用来构建计算图的函数，实际上是类`Function`的对象。该对象知道如何前向计算、如何在反向传播中计算其导数。\n",
    "\n",
    "对反向传播函数的引用存储在tensor属性`.grad_fn`中："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x000002DEF469AF40>\nGradient function for loss = <BinaryCrossEntropyWithLogitsBackward object at 0x000002DEF469A6D0>\n"
     ]
    }
   ],
   "source": [
    "print('Gradient function for z =',z.grad_fn)\n",
    "\n",
    "print('Gradient function for loss =', loss.grad_fn)"
   ]
  },
  {
   "source": [
    "## 6.2 计算梯度\n",
    "\n",
    "为了优化神经网络中的权重参数，我们需要计算出损失函数对参数的导数。即固定值x和y下的\n",
    "\n",
    "$$\n",
    "\\frac{\\partial loss}{\\partial w} \\; and \\; \\frac{\\partial loss}{\\partial b}\n",
    "$$\n",
    "\n",
    "使用`loss.backward()`，获得参数w和b的梯度："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.2406, 0.2332, 0.1253],\n        [0.2406, 0.2332, 0.1253],\n        [0.2406, 0.2332, 0.1253],\n        [0.2406, 0.2332, 0.1253],\n        [0.2406, 0.2332, 0.1253]])\ntensor([0.2406, 0.2332, 0.1253])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "source": [
    "Note:\n",
    "\n",
    "+ 只能获取计算图中叶子节点的梯度属性(这些叶子节点的requires_grad属性为True)，对于计算图中的其他节点，梯度都不可用。\n",
    "\n",
    "+ backward出于性能原因，在给定的计算图上只能使用一次梯度计算。如果需要在同一个计算图上多次调用backward，我们需要传递 retain_graph=True 给backward"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 6.3 关闭梯度跟踪\n",
    "\n",
    "默认情况下，所有张量的属性`requires_grad=True`，即默认跟踪它们的计算历史并支持梯度计算。\n",
    "\n",
    "但是，有些情况下我们不需要跟踪梯度，比如在验证集上检验模型性能的时候，即我们只想通过网络进行前向计算。这个时候，我们可以通过调用`torch.no_grad()`块包围我们的计算代码来停止跟踪计算:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nFalse\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b   \n",
    "print(z.requires_grad)"
   ]
  },
  {
   "source": [
    "关闭梯度跟踪的原因有：\n",
    "\n",
    "+ 将神经网络中的某些参数冻结，该场景常用于微调预训练模型\n",
    "\n",
    "微调预训练模型:https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "\n",
    "+ 仅需要前向计算的时候，加快计算速度，常用于测试模型在验证集上的表现\n",
    "\n",
    "从概念上讲，autograd 在由Function 对象组成的有向无环图 (DAG) 中保存数据（张量）和所有已执行操作（以及生成的新张量）的记录 。在这个 DAG 中，叶子是输入张量，根是输出张量。通过从根到叶跟踪此计算图，可以使用链式法则自动计算梯度。\n",
    "\n",
    "在前向传递中，autograd 同时做两件事：\n",
    "\n",
    "+ 计算结果张量\n",
    "\n",
    "+ 维护在 DAG 中的梯度函数。\n",
    "\n",
    "当`.backward()`在 DAG 的根节点上调用时，反向传递开始。它会：\n",
    "\n",
    "+ 计算来自每个`.grad_fn`的梯度\n",
    "\n",
    "+ 将它们累积在各自张量的`.grad`属性中\n",
    "\n",
    "+ 使用链式法则，一直传播到叶子张量。\n",
    "\n",
    "\n",
    "Note:\n",
    "\n",
    "PyTorch 中,DAG 是动态的。需要注意的是，该图是从头开始重新创建的；每次 .backward()调用后，autograd 开始填充新图形。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}